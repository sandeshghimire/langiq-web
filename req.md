Responsibilities
• Collaborate with customers and researchers to thoroughly understand business objectives
and technical constraints
• Lead the technical implementation of custom AI solutions, including prototyping,
deployment, and monitoring
• Deliver high-quality code and documentations in adherence to the highest engineering
standards
• Drive continuous improvements in solution design and deployment process
• Provide targeted training to clients - including non-technical stakeholders - when required


Key Technical Focus Areas

Core Responsibilities

Agentic RAG Pipelines: Design AI workflows combining retrieval-augmented generation with agent-based reasoning using vector search and semantic embeddings
Multi-Agent Systems: Create orchestrated AI agent architectures using frameworks like LangChain, LangGraph, or AutoGen
LLM Fine-Tuning: Lead model customization efforts including instruction tuning and RLHF for domain-specific optimization
MCP Integration: Implement Model Context Protocol servers for structured data access and enterprise system connectivity
AWS Cloud Architecture: Design scalable, secure cloud infrastructure for AI/ML workloads
Required Qualifications

Education & Experience

Advanced degree (MS/PhD) in Computer Science, ML, AI, or related field
5-10 years professional experience in software engineering, ML, or AI architecture
3+ years hands-on experience with large language models and advanced NLP systems
Technical Requirements

LLM/NLP Expertise: Experience with GPT-style models, transformer architectures, prompt engineering, and fine-tuning
RAG Implementation: Experience with vector databases such as Pinecone, Weaviate, Chroma, FAISS, and semantic search pipelines
Vector Databases: High-dimensional indexing, embedding models, and low-latency optimization
Multi-Agent Systems: Experience with LangChain Agents, LangGraph, AutoGen, and agent orchestration frameworks
Model Customization: Fine-tuning with HuggingFace, DeepSpeed, model evaluation, and bias mitigation
MCP/Tool Integration: External API/database integration and context-aware model extensions
Cloud (AWS): Experience with EC2, S3, Lambda, SageMaker, and containerization (Docker/Kubernetes)
Programming: Proficiency in Python, PyTorch/TensorFlow, and clean coding practices
Soft Skills

Systems thinking with ability to balance technical trade-offs (accuracy vs. latency, complexity vs. maintainability)
Strong communication skills for conveying AI concepts to non-technical stakeholders
Analytical problem-solving with creativity in architecting novel solutions
Preferred Qualifications (Differentiators)

Direct experience with LangChain/LangGraph/AutoGen frameworks
Knowledge of knowledge graphs and hybrid search implementations
AI in Finance background (though not required)
Research contributions (publications, open-source projects, patents)
AWS certifications (Machine Learning Specialty preferred)



help design experimental end-to-end ML/AI pipelines
contribute to addressing new use cases, beyond state of the art
improve/adapt AI pipelines for production, working directly with client data (often live data streams)
invent ways to pre-process data sources and perform tweaks (reranking, model paramater configuration...) for optimal performance of AI pipelines
design benchmark tasks and perform experiments
build unit tests and implement model monitoring
 contribute high-quality production code to our developer frameworks, used by thousands of developers
help to pre-process data sets for LLM training. 
Build and enhance React-based frontends hosted on CloudFront
Design scalable, low-latency APIs using FastAPI (Python) integrated with API Gateway (REST + WebSocket)
Develop AWS Lambda functions for backend services, data handling, and orchestration
Hands-on experience with OpenSearch for implementing scalable search functionality
Manage authentication using SSO, and enable secure access flows
Integrate real-time WebSocket interfaces for LLM streaming and dashboarding
Work closely with data science teams to connect LLM pipelines (LangChain + RAG) and vector search mechanisms
Design and maintain serverless data layers using DynamoDB, Aurora PostgreSQL, Athena, and S3
Participate in CI/CD automation using GitHub Actions, CloudFormation/CDK, and manage IAM roles/policies
Collaborate on data pipelines using AWS Glue, Airflow (MWAA), and Sagemaker for model training/inference Key Technologies
Frontend: React, TypeScript, CloudFront
Backend: FastAPI (Python), AWS Lambda, API Gateway (REST + WebSocket), Cognito
Search & Storage: OpenSearch, DynamoDB, Aurora PostgreSQL, S3, Athena
GenAI & RAG: LangChain, FAISS, Pinecone, OpenAI, Claude, AWS Bedrock
Data Engineering: AWS Glue, Airflow (MWAA)
DevOps: GitHub Actions, CloudFormation/CDK, IAM Roles
Streaming & Realtime: WebSockets, LLM streaming pipelines You Should Have
Proven experience in building production-grade GenAI features using LLM APIs and RAG pipelines
Hands-on experience with serverless app development on AWS
Solid understanding of FastAPI, microservices architecture, and REST/WebSocket API design
Comfort with both structured data (PostgreSQL) and NoSQL (DynamoDB)
Experience working in a CI/CD DevOps environment with infrastructure-as-code
Excellent problem-solving skills, and ability to troubleshoot complex cloud-native systems


What You’ll Work On

Optimize our LLM inference stack for lower latency and faster response times
Profile and accelerate multi-hop workflows:
RAG (VectorDB: Qdrant)
Web search (SERP APIs)
Orchestration (Mistral-based merging and reranking)
Tune and quantize models (e.g., BioPharma LLMS, vLLM, Ollama. GGUF, TGI)
Implement streaming & async pipelines for progressive response generation
Reduce overhead in multi-model orchestration
Collaborate with our core team on model routing, caching strategies, and LLM cost/performance tradeoffs


We’re Looking For Someone Who:

Has deep experience in LLMs, RAG, and inference optimization
Has worked with open-source models (LLaMA, Mistral, Opensource biopharma models, etc.)
Is comfortable with GPU-based model serving, quantization, and token-level profiling
Understands how to optimize real-world LLM latency (not just benchmarks)
Can work across our stack: Python, Node, MongoDB, Postgres, Qdrant, AWS
Bonus: Familiar with vLLM, TGI, Ollama, or LangChain-like tools


Key Responsibilities

Design and develop robust, scalable full-stack applications with a focus on generative AI and large language models.
Gather and analyze business and functional requirements, translating them into technical specifications.
Collaborate with different teams to implement best practices in architecture and design.
Produce comprehensive, usable software documentation.
Assist in infrastructure design and solution architecting, specifically within AWS environments.

Must-Have Skills:

Proficiency in Python and front-end development with React/node.js.
Extensive experience with AWS, including Lambda, Step Functions, DynamoDB, AppSync, Bedrock, SageMaker, and CloudWatch.
Strong understanding and practical experience in multi-threaded programming and utilizing generative AI large language models.

Industry Experience:

Strong background in developing and deploying serverless architectures following AWS Well Architected principles.
Experience in AI-based coding assistants for code development, test development, and documentation is preferred.
Proven track record in infrastructure design and solution architecting within cloud environments, with a strong emphasis on AWS.


As an LLM Architect at Aviz, you will be at the forefront of our AI innovation, applying LLM to solve real-world challenges in network infrastructure. You will:
Lead the development, training, and fine-tuning of LLMs to address complex networking problems and enhance automation, prediction, and decision-making.
Architect and build Network AI Assistants that deliver measurable ROI to infrastructure teams through intelligent insights, automation, and forecasting.
Collaborate closely with cross-functional teams, including data engineers, software developers, and customers, to ensure seamless integration of LLM capabilities into our products.
Research and implement the latest advancements in LLM modeling techniques, driving technical strategy to push the boundaries of what’s possible with AI.
Analyze, benchmark, and continuously improve LLM performance using metrics and customer feedback.
Stay ahead of emerging technologies and techniques in the LLM space, ensuring Aviz remains a leader in AI innovation.
What We’re Looking For
We’re looking for someone who thrives in a fast-paced, dynamic environment and is unafraid to take on bold challenges. The ideal candidate will have:
MS or PhD in Computer Science, Machine Learning, Artificial Intelligence, or a related field (or equivalent experience).
Deep expertise in Natural Language Processing (NLP) techniques, including tokenization, embeddings, transformers, etc.
Proven hands-on experience developing, training, and scaling LLMs such as LLAMA3 and others.
Strong programming skills in languages like Python and familiarity with modern deep learning frameworks, such as RAG, for fine-tuning models.
Experience with data pipelines, preprocessing, and managing large datasets for model training and deployment.
Proven ability to design and deliver software products incorporating machine learning at scale, with real-world impact.
Strong problem-solving skills and ability to work collaboratively in a fast-moving environment.
Fundamental understanding of distributed computing and key concepts in network infrastructure.


Design, build, and maintain robust, data-driven test automation frameworks JavaScript/TypeScript and Python.
Build automated evaluation pipelines and prompt-based tests for LLM and conversational AI systems.
Analyze large-scale, unstructured logs and telemetry data to identify issues and user experience gaps.
Create custom scripts to support test automation, log parsing, and performance evaluation of machine learning models.
Develop visual dashboards and reports to track quality, system health, and model behavior over time.
Support test environment setup and integrate with CI/CD pipelines for continuous testing.
Perform root cause analysis, debugging, and reporting of product issues, collaborating closely with Developers and SQEs across the SDLC.
Drive innovation by introducing new testing methodologies, tools, and best practices for both deterministic and non-deterministic systems.
Partner with global, cross-functional teams including ML engineers, product managers, and infrastructure leads.


What You’ll Do:

Fine-tune large language models for domain-specific conversation and recommendation
Design intelligent, tool-using agents with multi-step reasoning and memory
Implement and experiment with frameworks like LangChain, LangGraph, or Google’s Agent Developer Kit (ADK)
Integrate with Vertex AI and other foundation model platforms for inference, embeddings, and orchestration
Collaborate across front-end, backend, and AI teams to deliver seamless end-user experiences
Stay on the edge of LLM trends—streaming, multimodality, and interactive agents
What We’re Looking For:

4+ years of experience with LLMs, including model fine-tuning and deployment
Proficiency in Python and modern AI tooling (e.g., LangChain, LangGraph, OpenAI, Hugging Face)
Experience designing agent workflows, including context management, tool-calling, and chaining
Familiarity with Vertex AI, cloud APIs, or comparable ML platforms
Bonus: experience with real-time applications, embeddings, or multimodal input
Strong collaboration and communication skills in a remote, fast-paced environment